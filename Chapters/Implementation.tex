\chapter{Implementation}

This chapter details the implementation of the compiler, with the first few sections covering all
major phases from parsing to LLVM IR generation. Where appropriate, examples of how the language is
represented at each stage of the compilation process will be provided. The final sections of this
chapter will specifically cover the implementation and handling of closures and higher-order
functions, as these language features first require the full understanding of the implementation to
be explained effectively.

\section{Parsing}

As the library used for parsing, \emph{FastParse}, is a parser combinator library, the parser is
implemented similarly to the grammar, with each non-terminal symbol being implemented as a function
that returns a parser. The parser output is mapped to an abstract syntax tree (AST), represented by
Scala case classes.

A condensed example can be seen as follows:

\begin{code}{scala}
    sealed trait Exp
    case class Call(name: String, args: Seq[Exp]) extends Exp
    case class Num(i: Int) extends Exp
    case class Bool(b: Boolean) extends Exp
    case class Var(s: String) extends Exp

    def NumParser[$: P] =
        P(CharsWhileIn("0-9", 1)).!.map(_.toInt)

    def BoolParser[$: P] =
        P("true").map(_ => true) |
        P("false").map(_ => false)

    def IdParser[$: P] = P(
        !StringIn("if", "then", "else", "print", "def", "val", "enum", "struct", "true", "false")
        ~ CharIn("A-Za-z_") ~~ CharsWhileIn("A-Za-z0-9_", 0)
    ).!

    def Primary[$: P]: P[Exp] = P(
        (IdParser ~ "(" ~ Exp.rep(0, ",") ~ ")").map(Call) |
        NumParser.map(Num) |
        BoolParser.map(Bool) |
        IdParser.map(Var)
    )
\end{code}

\noindent where \texttt{IdParser}, \texttt{NumParser} and \texttt{BoolParser} define the lexical
structure of identifiers, numbers and booleans respectively. This parser allows for the following
transformation between the input and the AST:

\begin{tcolorbox}
    \begin{minted}{scala}
        // input
        foo(6 + bar(4), 2)
    \end{minted}
    \tcblower
    \begin{minted}{scala}
        // AST
        Call("foo", Seq(
            Op(
                Num(6),
                "+",
                Call("bar", Seq(Num(4)))
            ),
            Num(2)
        ))
    \end{minted}
\end{tcolorbox}

% \begin{figure}[h]
%     \begin{center}
%         \begin{tikzpicture}[
%                 node distance=4cm,
%                 font=\small
%             ]
%             \node (input) {
%                 \begin{minipage}{0.29\textwidth}
%                     \begin{code}{text}
%                         foo(6 + bar(4), 2)
%                     \end{code}
%                 \end{minipage}
%             };

%             \node (output) [right=of input] {
%                 \begin{minipage}{0.36\textwidth}
%                     \begin{code}{scala}
%                         Call("foo", Seq(
%                             Op(
%                                 Num(6),
%                                 "+",
%                                 Call("bar", Seq(
%                                     Num(4)
%                                 ))
%                             ),
%                             Num(2)
%                         ))
%                     \end{code}
%                 \end{minipage}
%             };

%             \draw[shorten >=0.2cm, shorten <=0.2cm,-Latex] (input) -- (output) node[midway, above]
%             {Parser};
%         \end{tikzpicture}
%     \end{center}
%     \caption{An example of parsing input to AST.}
% \end{figure}

A particular implementation detail of the parser is in handling operator precedence. A common
grammar for parsing with operator precedence, notated in Extended Backus-Naur Form (EBNF), is as
follows:

\begin{center}
    \begin{grammar}
        <expr> ::= <term>
        \alt <expr> `+' <term>
        \alt <expr> `-' <term>

        <term> ::= <factor>
        \alt <term> `*' <factor>
        \alt <term> `/' <factor>

        <factor> ::= <number>
        \alt `(' <expr> `)'
    \end{grammar}
\end{center}

While this correctly handles operator precedence, it does not handle left-associativity of
operators if implemented naively. For example, the grammar above would parse
\(1 - 2 - 3 ~[= -4]\) as \(1 - (2 - 3) ~[=2]\),
which is not the desired behaviour. To handle this, the grammar must be modified to include
left-associative rules:

\begin{grammar}
    <expr> ::= <term> \{ (`+' | `-') <term> \}

    <term> ::= <factor> \{ (`*' | `/') <factor> \}

    <factor> ::= <number>
    \alt `(' <expr> `)'
\end{grammar}

Using the semantic actions of parser combinators, a left-associative transformation can be
applied to the parsed output. This is achieved by defining a recursive function that takes the
parsed output and applies the left-associative transformation. The implementation of this using
\emph{FastParse} is as follows:

\begin{code}{scala}
    def lft(a: Exp, b: Seq[(String, Exp)]): Exp = (a, b) match {
        case (`a`, (b, c) :: next) => lft(Op(a, b, c), next)
        case _ => a
    }

    def Expr[$: P]: P[Exp] =
        (Term ~ (CharIn("+\\-").! ~ Term).rep).map(lft(_,_))

    def Term[$: P]: P[Exp] =
        (Factor ~ (CharIn("/*%").! ~ Factor).rep).map(lft(_,_))

    def Factor[$: P]: P[Exp] =
        NumParser.map(Num) |
        "(" ~ Expr ~ ")"
    ...
\end{code}

As mentioned in Chapter~\ref{sec:design}, care was taken to ensure the grammar created was
unambiguous and not left-recursive. The final language grammar specification is available in
Appendix~\ref{app:grammar}.

\section{IR Generation}

After generating the AST from the parser, it is converted to the ANF IR as a series of
let-expressions, defining the scope of variables and their values. This can be seen as equivalent to
lambda abstractions in the lambda calculus. The conversion is performed via a Continuation-Passing
Style function. Consider the following program and it's ANF-style intermediate data representation:

\begin{tcolorbox}
    \begin{minted}{scala}
        // program
        foo(6 + bar(4), 2)
    \end{minted}
    \tcblower
    \begin{minted}{scala}
        // ANF IR
        Let("tmp0", Call("bar", [Num(4)]),
            Let("tmp1", Op(Num(6), "+", Var("tmp0")),
                Let("tmp2", Call("foo", [Var("tmp1"), Num(2)])
                    Return(Var("tmp2"))
                )
            )
        )
    \end{minted}
\end{tcolorbox}

% \begin{figure}[h]
%     \begin{center}
%         \begin{tikzpicture}[
%                 node distance=1cm,
%                 font=\small
%             ]
%             \node (input) {
%                 \begin{minipage}{0.29\textwidth}
%                     \begin{code}{text}
%                         foo(6 + bar(4), 2)
%                     \end{code}
%                 \end{minipage}
%             };

%             \node (output) [right=of input] {
%                 \begin{minipage}{0.70\textwidth}
%                     \begin{code}{scala}
%                         Let("tmp0", Call("bar", [Num(4)]),
%                             Let("tmp1", Op(Num(6), "+", Var("tmp0")),
%                                 Let("tmp2", Call("foo", [Var("tmp1"), Num(2)])
%                                     Return(Var("tmp2"))
%                                 )
%                             )
%                         )
%                     \end{code}
%                 \end{minipage}
%             };

%             \draw[shorten >=0.2cm, shorten <=0.2cm,-Latex] (input) -- (output);
%         \end{tikzpicture}
%     \end{center}
%     \caption{A representation of ANF IR for the program on the left}.
% \end{figure}

Observe that the deeper expressions appear before shallower ones in the ANF
representation. A simple recursive solution for converting the program to ANF would not work as this
structure cannot be created bottom-up. It can be seen that the later expressions of the IR depend on
the expressions constructed earlier on, such as \texttt{tmp2} depending on \texttt{tmp1}, which in
turn depends on \texttt{tmp0}.

To solve this `inversion' of calculating values, CPS allows for passing a continuation function
representing the context in which a value is used. This allows for the expression to remain
incomplete until the value it depends on is assigned to a variable.

The IR generation function as implemented takes an AST expression and a continuation function, that
takes a value and returns an ANF expression. The CPS function itself then returns the full program
under an ANF expression. A simplified version of the CPS function is as follows:

\begin{code}{Scala}
    def CPS(e: Exp)(k: KVal => KAnf) : KAnf = e match {
        // Values are passed to the continuation function
        case Var(s) => k(KVar(s))
        case Num(i) => k(KNum(i))
        case Bool(b) => k(KBool(b))
        case Flt(f) => k(KFloat(f))
        ...
        // Other expressions are handled by creating a new continuation function
        case Op(e1, o, e2) => {
            val z = Fresh("tmp")
            CPS(e1)((y1) =>
                CPS(e2)((y2) =>
                    KLet(z, Kop(o, y1, y2), k(KVar(z)))
                )
            )
        }
        case Func(name, args, body) =>
            KFun(name, args, CPS(body)((y) => KReturn(y)), k(KVar(name)))
        case StructDef(name, items) =>
            KStructDef(Struct(name, items), k(KVar(name)))
        ...
    }
\end{code}

The \texttt{Fresh} function is used to generate a globally unique name for each temporary variable
introduced, in order to fulfill the SSA requirements that LLVM IR has.

\subsection{Handling typing}

As the LLVM IR is statically typed, the ANF IR to be generated must also be typed. While more
complex type systems exist, such as Hindley-Milner type inference, the type system used in this
project heavily leans on the existing type system of LLVM IR, with the programmer being required to
annotate the types of variables at declaration.

These types are then propagated through the ANF IR generation process via a type environment, which
is a map from variable names to their types. The type environment is updated as new variables are
introduced, and the types of expressions are checked against the type environment. As the type
environment is recursively passed through the function, the scope of the type environment will
naturally reflect available types at any given point in the program. Using the example from earlier,
an updated version of the CPS function that includes type propagation is as follows:

\begin{code}{scala}
    type TypeEnv = Map[String, Type]

    def CPS(e: Exp, ty: TypeEnv)(k: (KVal, TypeEnv) => KAnf) : KAnf = e match {
        // Values and their types are passed to the continuation function
        case Var(s) => k(KVar(s, ty(s)), ty)
        case Num(i) => k(KNum(i), ty)
        case Bool(b) => k(KBool(b), ty)
        case Flt(f) => k(KFloat(f), ty)
        ...
        // Other expressions are handled by creating a new continuation function
        case Op(e1, o, e2) => {
            val z = Fresh("tmp")
            CPS(e1, ty)((y1, t1) =>
                CPS(e2, t1)((y2, t2) =>
                    KLet(z, Kop(o, y1, y2), k(KVar(z, y1.get_type), t2))
                )
            )
        }
        case Func(name, args, ret, body) => {
            // update the type environment for the function body
            val body_ty = ty
                ++ args.map{case (x, t) => (x, t)}
                + (name -> FnType(args.map(_._2).toList, ret))
            // update the type environment for after the function itself
            val return_ty = ty
                + (name -> FnType(args.map(_._2).toList, ret))
            KFun(name, args, ret, CPS(body, body_ty)((y, _) => KReturn(y)), k(KVar(name), return_ty))
        }
        case StructDef(name, items) =>
            // update the ty with struct type, as well as items inside the struct
            val updated_ty = ty
                + (name -> UserType(name))
                ++ items.map{case (x, t) => (s"$name.$x", t)}
            KStructDef(Struct(name, items), k(KVar(name), updated_ty))
        ...
    }
\end{code}

\section{Enumerated Types and Pattern Matching}

% Need to contend with how to compile the definition, and the reference of an enum.
The implementation of enums and pattern matching requires the consideration of two main aspects:
\begin{enumerate}
    \singlespacing
    \item The definition of enums and their values.
    \item Referencing enums in comparisons and pattern matching expressions.
\end{enumerate}
Consider the following program containing an enum definition and a pattern matching expression:

\begin{code}{scala}
    enum Days = Spring | Summer | Autumn | Winter;

    def main() = {
        val contrivedExample: Season = Season::Spring;
        print(contrivedExample match {
            case Season::Spring => true
            case Season::Autumn => true
            case _ => false
        });
        0
    }
\end{code}

The LLVM IR does not have a native representation of enums or pattern matching. To accommodate enums
and pattern matching, a pre- and post-processing step is introduced to the ANF IR generation.

The pre-processing step is responsible for extracting all enum definitions from the AST, collecting
them into a map from enum names to their possible values, and transforming match expressions into a
series of if-else expressions. This step is done prior to the ANF IR generation as it eliminates the
need to handle the pattern matching transformation within the ANF IR generation itself. Using the
example above, the pre-processing step would transform the program into:

\begin{code}{scala}
    def main() = {
        val contrivedExample: Season = Season::Spring;
        print(
            if (contrivedExample == Season::Spring) {
                true
            } else if (contrivedExample == Season::Autumn) {
                true
            } else {
                false
            }
        );
        0
    }
\end{code}

This step is implemented as a series of recursive functions that traverse the AST and transform the
program accordingly.

\begin{code}{scala}
    // Removes Enum definitions from the AST and returns them seperately
    def extract_enums(e: Exp) : (Map[String, Type], Exp) = e match {
        case Sequence(e1: EnumDef, e2) =>
            val (enums, e_) = extract_enums(e2)
            (enums + (e1.name -> EnumType(e1.vals)), e_)
        // ... Other recursive cases ...
        case e: EnumDef => (Map(e.name -> EnumType(e.vals)), e)
        case e => (Map(), e)
    }

    // Convert match statements to if-else statements
    def transform_match_to_if(e: Exp) : Exp = e match {
        case Match(a, cases) =>
            // Fold start value is a no-op (0 + 0) if no default case is given
            cases.foldRight[Exp](Op(Num(0), "+", Num(0)))((mcase, acc) => {
                val MCase(root, item, exp) = mcase
                if root == "" && item == "_" then
                    transform_match_to_if(exp)    // default
                else
                    If(Op(Var(a), "==", EnumRef(root, item)), transform_match_to_if(exp), Some(acc))
            })
        // ... Other recursive cases ...
        case e => e
    }
\end{code}

The post-processing step is responsible for replacing enum references with their corresponding
integer values, using the map generated in the pre-processing step. The integer value assigned to
each enum value is determined by the index in which they are defined in the enum definition,
identical to the behaviour of C and C++ enums. This step is done after the ANF IR generation to
allow for the specific enum types to be properly propagated through the ANF IR\@. The
post-processing step would transform the program into\footnote{For illustration purposes, this
transformation is shown as the program equivalent to the resulting ANF IR\@. In reality, the
post-processing step would transform the ANF IR directly.}:

\begin{code}{scala}
    def main() = {
        val contrivedExample: Int = 0;
        print(
            if (contrivedExample == 0) {
                true
            } else if (contrivedExample == 2) {
                true
            } else {
                false
            }
        );
        0
    }
\end{code}

This step is also implemented as a series of recursive functions that traverse the ANF IR, with the
enum replacement carried out as a simple map lookup.

\begin{code}{scala}
    // ...
    case v: KEnum =>
        en_map.get(v.root) match {
            // check if enum is in map
            case Some(EnumType(items)) =>
                val idx = items.indexOf(v.item)
                // check if valid enum value
                if idx == -1 then
                    throw new Exception("Undefined enum value")
                KNum(idx)
            case _ => throw new Exception("Undefined enum type")
        }
    // ...
\end{code}

At this point, all references to enums have been removed, and the program is in a form that can
almost directly be translated to LLVM IR.

\section{LLVM IR Generation}

After both the ANF IR generation and post-processing steps, the state of the program is such that
the chain of let-expressions encompasses the entire program, with the final continuation function
being the return statement. This includes the definitions of functions, structs and enums, as well
as the main function itself. To extract certain constructs like function definitions from these
let-expressions, a recursive function is used to hoist them out and into the top-level of the
program. This is done to ensure that definitions are available globally, as they are in the source
language.

\begin{code}{scala}
    def hoist(e: KAnf): (List[CFunc], KAnf, List[Env], List[Struct]) = e match {
        case KFun(fnName, args, ret, body, next) => {
            val (fns, e, envs, structs) = hoist(body)
            val (fns2, e2, envs2, structs2) = hoist(next)
            val entry = Fresh("entry")
            val fn = CFunc(fnName, args, ret, e)
            (fn :: fns ::: fns2, e2, envs ::: envs2, structs ::: structs2)
        }
        case KIf(x1, e1, e2) => {
            val (fns, t, envs, structs) = hoist(e1)
            val (fns2, f, envs2, structs2) = hoist(e2)
            val thn = Fresh("then")
            val els = Fresh("else")
            (fns ::: fns2, KIf(x1, t, f), envs ::: envs2, structs ::: structs2)
        }
        case KLetEnv(x, env: Env, next) => {
            val (fns, e1, envs, structs) = hoist(next)
            (fns, KLetEnv(x, env, e1), env :: envs, structs)
        }
        case KLet(x, v, next) => {
            val (fns, e1, envs, structs) = hoist(next)
            (fns, KLet(x, v, e1), envs, structs)
        }
        case KConst(x, v, e) => {
            val (fns, e1, envs, structs) = hoist(e)
            (fns, KConst(x, v, e1), envs, structs)
        }
        case KStructDef(struct, e) => {
            val (fns, e1, envs, structs) = hoist(e)
            (fns, KStructDef(struct, e1), envs, struct :: structs)
        }
        case _ => (Nil, e, Nil, Nil)
    }
\end{code}

The final step of the compilation process is the generation of LLVM IR from the ANF IR\@. This is
done by traversing the ANF IR and emitting the corresponding LLVM IR for each construct.

TBC

\section{Composite Types}

The implementation of composite types (structs) requires the handling of three main concerns:

\begin{enumerate}
    \singlespacing
    \item The definition of structs and their members.
    \item The allocation of memory for a new instance of a struct.
    \item The referencing of struct members.
\end{enumerate}

Despite structs sharing the same two concerns as enums, the implementation detail is quite different
from that of enums, as the former requires the generation of a new type and the allocation of memory
for the struct in the LLVM IR\@. The following program demonstrates the use of structs:

\begin{code}{scala}
    struct Point = {
        x: Int,
        y: Int
    };

    def main() = {
        val p: Point = Point(1, 2);
        print(p.x + p.y);
        0
    }
\end{code}

After the ANF IR generation, the struct definition is enclosed within the let-expression
encompassing the entire program. As with function definitions, the struct definition is hoisted out
into the top-level of the program.

Structs in the LLVM IR are represented as a series of elements, with each element being a member of
the struct. These elements do not have names as in the source language, but are instead accessed by
their index. It is therefore important to keep track of the order of elements in the struct
definition throughout generation. The struct definition is then emitted as a new type in the LLVM
IR\@, like so:

\begin{tcolorbox}
    \begin{minted}{scala}
        // Program
        struct Point = {
            x: Int,
            y: Int
        };
    \end{minted}
    \tcblower
    \begin{minted}{llvm}
        ; LLVM IR
        %Point = type { i32, i32 }
    \end{minted}
\end{tcolorbox}

When creating a new instance of a struct, memory must be allocated for it. Memory allocation in the
LLVM IR can either be done on the stack or the heap. Allocating to the stack provides a simpler and
efficient solution, as the memory is automatically deallocated when a function returns. However,
this raises implications for the lifetime of the struct, as it is only valid within a narrow scope
as determined by the stack. Allocating to the heap (e.g. \texttt{malloc} in C) provides a more
flexible solution, as the memory is valid for the lifetime of the program, but requires either
explicit manual deallocation or the creation of a garbage collector.

For the purposes of this project, memory allocation is done on the stack using the \texttt{alloca}
instruction in the LLVM IR, returning a pointer to the allocated memory. For each element in the
struct to be allocated, a \texttt{getelementptr} instruction is used to calculate the address of the
element within the struct via its index, and a \texttt{store} instruction is used to store the value
of the element. The following is the LLVM IR for the creation of a new instance of the
\texttt{Point} struct:

\begin{tcolorbox}
    \begin{minted}{scala}
        // Program
        val p: Point = Point(1, 2);
    \end{minted}
    \tcblower
    \begin{minted}{llvm}
        ; LLVM IR
        %p = alloca %Point
        %tmp0 = getelementptr %Point, %Point* %p, i32 0, i32 0
        store i32 1, i32* %tmp0
        %tmp1 = getelementptr %Point, %Point* %p, i32 0, i32 1
        store i32 2, i32* %tmp1
    \end{minted}
\end{tcolorbox}

The referencing of struct members is done by using the \texttt{getelementptr} instruction to
calculate the address of the struct member via its index, and the \texttt{load} instruction to load
the value of the member into a local variable. To access the \texttt{x} and \texttt{y} members of
the \texttt{Point} struct, the following LLVM IR is generated:

\begin{tcolorbox}
    \begin{minted}{scala}
        // Program
        p.x + p.y
    \end{minted}
    \tcblower
    \begin{minted}{llvm}
        ; LLVM IR
        %tmp2 = getelementptr %Point, %Point* %p, i32 0, i32 0
        %tmp3 = load i32, i32* %tmp2
        %tmp4 = getelementptr %Point, %Point* %p, i32 0, i32 1
        %tmp5 = load i32, i32* %tmp4
        %tmp6 = add i32 %tmp3, %tmp5
    \end{minted}
\end{tcolorbox}

\section{Closures and Higher-order functions}

TBC

\begin{itemize}
\item go through prog, find functions
\item detect if free variables
\item if yes, closure.
\item create environment,
\item rewrite body of closure to access environment
\item return environment/fn ptr instead
\item fix types
\item fix calls to closure
\item hoist
\item create struct for closure
\item getelementmptr, load, ptr to ptr to fn
\end{itemize}
